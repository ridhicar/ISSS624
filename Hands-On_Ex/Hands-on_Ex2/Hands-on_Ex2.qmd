---
title: "Global and Local Measures of Spatial Autocorrelation"
author: "Ridhica Rangarajan"
editor: visual
---

## 1. Overview

This document covers the in-class exercise 1. In this exercise, we will learn how to compute spatial weights using R.

## 2. Getting Started

The code check below will install and load the relevant packages. **pacman** is a wrapper in R that helps us load and install packages.

-   Functions of **tidyverse** can be used to perform data science tasks such as importing, wrangling, and visualizing data.

-   Function of **readr** package (part of tidyverse) can be used to import csv files.

-   ***st_read(dsn = "file path", layer = "file name")*** of **sf** package can be used to import geospatial data.

-   Join function i.e., ***left_join(dataframe1, dataframe2)*** of **dplyr** can be used to perform relational join.

-   ***poly2nb()*** of **spdep** can be used to compute spatial weights and to calculate spatially lagged variables.

```{r}
# same as:
# library(pacman)
# p_load(tidyverse, sf ,..)
pacman::p_load(tidyverse, sf, tmap, spdep)
```

## 3. Importing Geospatial Data

### 3.1. Importing polygon features

This code chunk uses **st_read()** function of **sf** package to import Hunan county boundary layer ESRI shapefile into R as a polygon feature data frame.

st_read() uses two arguments:

-   `dsn`to define the data path

-   `layer` to provide the shapefile name

```{r}
hunan_sf = st_read(dsn = "data/geospatial", 
                  layer = "Hunan")
```

The message above reveals that the geospatial objects are multipolygon features. There are a total of 88 multipolygon features and 7 fields in `hunan` simple feature data frame. `hunan` is in **WGS 84** projected coordinates systems. The bounding box provides the x extend and y extend of the data.

## 4. Checking the Content of A Simple Feature Data Frame

### 4.1. Working with *st_geometry()*

The column in the sf data.frame that contains the geometries is a list, of class `sfc`. We can retrieve the geometry list-column in this case by hunan\$geom or hunan\[\[1\]\], but the more general way uses *st_geometry()* as shown in the code chunk below.

```{r}
st_geometry(hunan_sf)
```

### 4.2. Working with glimpse()

**glimpse()** report of **dplyr** reveals the data type of each fields. For example, `Shape Area` `Shape Length` fields are **double-precision values,** `ID_3` field is in **integer** data type.

```{r}
glimpse(hunan_sf)
```

### 4.3. Working with *head()*

```{r}
# To fetch the first 5 rows of hunan dataset.
head(hunan_sf, n=5)
```

The below code chunk uses ***st_crs()*** of **sf** package pulls the coordinate system of `hunan_sf`.

```{r}
st_crs(hunan_sf)
```

`hunan_sf` data frame is projected in WGS 84, when we read until the end of the print, it indicates that the EPSG is 4326. This is the correct EPSG code for WGS 84, it should be [4326](https://support.virtual-surveyor.com/en/support/solutions/articles/1000261351-what-is-wgs84-#:~:text=The%20WGS84%20Coordinate%20Systems%20adds,EPSG%20code%2C%20which%20is%204326.). Hence, there is no need for correction, use of *st_set_crs()* of **sf** package.

## 5. Plotting Geospatial Data

The code chunk below pulls multi-plot of all attributes.

```{r}
plot(hunan_sf)
```

The code chunk below plots only the geometry.

```{r}
plot(st_geometry(hunan_sf))
```

## 6. Importing Attribute (Aspatial) Data in csv into R

This code chunks below will import Dictionary.xlsx (optional) and Hunan_2012.csv into R. We will use ***read_csv()***of **readr** package to import Hunan_2012.csv. The output is R dataframe class called `hunan2012`.

```{r}
dict <- readxl::read_excel("/Users/ridz/ridhicar/ISSS624/In-class_Ex/In-class_Ex1/data/aspatial/Dictionary.xlsx")
```

```{r}
hunan2012 <- read_csv("/Users/ridz/ridhicar/ISSS624/In-class_Ex/In-class_Ex1/data/aspatial/Hunan_2012.csv")
```

### 6.1 Working with *list()*

The code chunk below shows ***list()*** of Base R instead of *glimpse()*, used to examine if the data file, `hunan2012` has been imported correctly.

```{r}
list(hunan2012)
```

The output reveals that `hunan2012` tibble data frame consists of 88 rows and 29 columns.\

## 7. Performing relational join

The code chunk below uses ***left_join(dataframe1, dataframe2)*** of **dplyr** package to create a new table `hunan`, joining the attribute table of hunan's SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe.

```{r}
hunan <- left_join(hunan_sf, hunan2012)
```

## 8. Visualising Regional Development Indicator

The code chunk below draws a chloropeth map showing the distribution of GDPPC 2012 by using ***qtm()*** of **tmap** package.

```{r}
equal <- tm_shape(hunan) +
  tm_fill("GDPPC",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal interval classification")

quantile <- tm_shape(hunan) +
  tm_fill("GDPPC",
          n = 5,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal quantile classification")

tmap_arrange(equal, 
             quantile, 
             asp=1, 
             ncol=2)
```

## 9. Global Spatial Autocorrelation

In this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.

## 9. Computing Contiguity Spatial Weights

We will use ***poly2nb()*** of **spdep** package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.

### 9.1 Computing (QUEEN) contiguity based on neighbours

The code chunk below is used to compute the Queen contiguity weight matrix. The "queen" argument takes TRUE (default) or FALSE as option. The function will return a list of first order neighbours using the Queen criteria.

```{r}
wm_q <- poly2nb(hunan, queen=TRUE)
summary(wm_q)
```

The summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.

For each polygon in our polygon object, *wm_q* lists all neighboring polygons. For example, to see the neighbors for the 11th polygon in the object, type:

```{r}
wm_q[[11]]
```

Polygon 11 has 3 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.

Using the code chunk below, we can retrieve the county name of Polygon ID=11:

```{r}
hunan$County[11]
```

The output reveals that Polygon ID=11 is Guidong county.

The code chunk below is used to reveal the county names of the three neighboring polygons:

```{r}
hunan$NAME_3[c(14,17,72)]
```

The code chunk below is used to retrieve the GDPPC of these three countries i.e., "Rucheng", "Zixing", "Yanling":

```{r}
# nb11 <- to see the neighbors for the 11th polygon in the object
nb11 <- wm_q[[11]]
# To retrieve the GPPPC of Polygon ID=11
nb11 <- hunan$GDPPC[nb11]
nb11
```

The printed output above shows that the GDPPC of the three nearest neighbours based on Queen's method are 11286, 65706 and 21021 respectively.

Similarly, to see the neighbors for the first polygon in the object, type:

```{r}
wm_q[[1]]
```

Polygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.

The code chunk below retrieves the county name of Polygon ID=1:

```{r}
hunan$County[1]
```

The output reveals that Polygon ID=1 is Anxiang county.

The code chunk below is used to reveal the county names of the five neighboring polygons:

```{r}
hunan$NAME_3[c(2,3,4,57,85)]
```

The code chunk below is used to retrieve the GDPPC of these three countries i.e., "Hanshou", "Jinshi", "Li", "Nan", "Taoyuan":

```{r}
# nb1 <- to see the neighbors for the first polygon in the object
nb1 <- wm_q[[1]]
# To retrieve the GPPPC of Polygon ID=1
nb1 <- hunan$GDPPC[nb1]
nb1
```

The printed output above shows that the GDPPC of the three nearest neighbours based on Queen's method are 20981, 34592, 24473, 21311 and 22879 respectively.

The code chunk below displays the complete weight matrix by using ***str()***.

```{r}
str(wm_q)
```

### 9.2 Creating (ROOK) contiguity based on neighbours

The code chunk below is used to compute Rook contiguity weight matrix. The "queen" argument takes TRUE (default) or FALSE as option. Setting "queen" to FALSE means Rook contiguity weight matrix.

```{r}
wm_r <- poly2nb(hunan, queen=FALSE)
summary(wm_r)
```

The summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one neighbours.

## 10. Row-standardized weights matrix

Next, we need to assign weights to each neighbouring polygon. In our case, each neighbouring polygon will be assigned equal weight (style="W"). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors' values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we'll stick with the style="W" option for simplicity's sake but note that other more robust options are available, notably style="B".

The zero.policy=TRUE option allows for lists of non-neighbours. This should be used with caution as the user may not be aware of missing neighbours in his/her dataset However, a zero.policy = FALSE will return an error, see below.

```{r}
rswm_q <- nb2listw(wm_q, style="W", zero.policy = TRUE)
rswm_q
```

The input of *nb2listw()* must be an object of class **nb**. The syntax of the function has two major arguments, namely style and zero.poly.

-   *style* can take values "W", "B", "C", "U", "minmax" and "S". B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).

-   If *zero policy* is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %\*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.

## 11.1 Global Spatial Autocorrelation: Moran's I

In this section, we will learn how to perform Moran's I statistical testing by using ***moran.test()*** from **spdep** package.

### 11.1.1 Maron's I test

The code chunk below performs Moran's I statistical testing by using ***moran.test()*** from **spdep** package.

```{r}
moran.test(hunan$GDPPC, 
           listw=rswm_q, 
           zero.policy = TRUE, 
           na.action=na.omit)
```

Moran I (Z value) is:

-   positive (I\>0): Clustered, observations tend to be similar;

-   negative (I\<0): Dispersed, observations tend to be dissimilar;

-   approximately zero: observations are arranged randomly over space

The p-value from Moran's I test is less than 0.05, this means that the test is statistically significant, we will reject the null hypothesis.

### 11.1.2 Computing Monte Carlo Moran's I

The code chunk below performs the permutation test on Moran's I statistic by using ***moran.mc()*** from **spdep** package. A total of 1000 stimulations will be performed.

```{r}
set.seed(1234)
bperm= moran.mc(hunan$GDPPC, 
                listw=rswm_q, 
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
bperm
```

The p-value from Moran's I test is less than 0.05, we will reject the null hypothesis.

### 11.1.3 Visualizing Monte Carlo Moran's I

The code chunks below examine the stimulated Moran's I test statistics.

```{r}
mean(bperm$res[1:999])
```

```{r}
var(bperm$res[1:999])
```

```{r}
summary(bperm$res[1:999])
```

```{r}
hist(bperm$res, 
     freq=TRUE, 
     breaks=20, 
     xlab="Simulated Moran's I")
abline(v=0, 
       col="red") 
```

From the histogram above that shows the Moran's I stimulation, we can see that the graph is more or less normally distributed.

```{r}
df <- data.frame(bperm$res)
df
```

```{r}
library(ggplot2)
# Basic histogram
ggplot(df, aes(x=bperm$res)) + geom_histogram()
# Change the width of bins
#ggplot(df, aes(x=bperm$res)) +
#  geom_histogram(binwidth=0.5)
# Change colors
p<-ggplot(df, aes(x=bperm$res)) + 
  geom_histogram(color="black", fill="white")
# Add mean line
p+ geom_vline(aes(xintercept=mean(bperm$res)),
            color="blue", linetype="dashed", size=1)
# Histogram with density plot
ggplot(df, aes(x=bperm$res)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666") 
p
```

## 11.2 Global Spatial Autocorrelation: Geary's

In this section, we will learn how to perform Geary's c statistical testing by using appropriate functions from **spdep** package.

### 11.2.1 Geary's C test

The code chunk below performs Geary's C test for spatial autocorrelation by using ***geary.test()*** from **spdep** package.

```{r}
geary.test(hunan$GDPPC, listw=rswm_q)
```

The p-value from Geary's C is less than 0.05, we will reject the null hypothesis.

Geary C (Z value):

-   Large c value i.e., \>1: Dispersed, observations tend to be dissimilar.

-   small c value i.e., \<1: Clustered, observations tend to be similar.

-   c = 1: observations are arranged randomly over space.

### 11.2.1 Computing Monte Carlo Geary's C

The code chunk below performs permutation test by Geary's C statistic by using ***geary.mc()*** from **spdep** package.

```{r}
set.seed(1234)
bperm=geary.mc(hunan$GDPPC, 
               listw=rswm_q, 
               nsim=999)
bperm
```

The p-value from Monte-Carlo stimulation of Geary's C is less than 0.05, we will reject the null hypothesis.

### 11.2.3 Visualizing the Monte Carlo Geary's C

The code chunks below are used to visualize the distribution of the stimulated values.

```{r}
mean(bperm$res[1:999])
```

```{r}
var(bperm$res[1:999])
```

```{r}
summary(bperm$res[1:999])
```

```{r}
hist(bperm$res, freq=TRUE, breaks=20, xlab="Simulated Geary c")
abline(v=1, col="red") 
```

The histogram is symmetrical with mean = 1.0.

## 12. Spatial Correlogram

Spatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran's I or Geary's c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.

### 12.1 Compute Moran's I correlogram

The code chunk below uses ***sp.correlogram()*** from the **spdep** package to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran's I. The **plot()** of base Graph is then used to plot the output.

```{r}
MI_corr <- sp.correlogram(wm_q, 
                          hunan$GDPPC, 
                          order=6, 
                          method="I", 
                          style="W")
plot(MI_corr)
```

By plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.

```{r}
print(MI_corr)
```

??

### 12.2 Compute Geary's C correlogram and plot

The code chunk below uses ***sp.correlogram()*** from the **spdep** package to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary's C. The **plot()** of base Graph is then used to plot the output.

```{r}
GC_corr <- sp.correlogram(wm_q, 
                          hunan$GDPPC, 
                          order=6, 
                          method="C", 
                          style="W")
plot(GC_corr)
```

```{r}
print(GC_corr)
```

## 13. Cluster and Outlier Analysis

Local Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.

In this section, we will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran'I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.

### 13.1 Computing local Moran's I

we will use ***localmoran()*** function from **spdep** to compute the local Moran's I of GDPPC2012. It computes *Ii* values, given a set of *zi* values and a listw object providing neighbour weighting information for the polygon associated with the zi values.

```{r}
fips <- order(hunan$County)
localMI <- localmoran(hunan$GDPPC, rswm_q)
head(localMI)
```

*localmoran()* function returns a matrix of values whose columns are:

-   Ii: the local Moran's I statistics

-   E.Ii: the expectation of local moran statistic under the randomisation hypothesis

-   Var.Ii: the variance of local moran statistic under the randomisation hypothesis

-   Z.Ii:the standard deviate of local moran statistic

-   Pr(): the p-value of local moran statistic

The code chunk below is used to list the content of the local Moran's matrix. This is derived by using ***printCoefmat()***.

```{r}
printCoefmat(data.frame(
  localMI[fips,], 
  row.names=hunan$County[fips]),
  check.names=FALSE)
```

### 13.1.1 Mapping the local Moran's I

Before mapping the local Moran's I map, it is wise to append the local Moran's I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called *hunan.localMI*.

```{r}
hunan.localMI <- cbind(hunan,localMI) %>%
  rename(Pr.Ii = Pr.z....E.Ii..)
```

### 13.1.2 Mapping the local Moran's I values

The code chunk below uses choloropeth mapping functions **tmap** pacakge to plot the local Moran's I value.

```{r}
tm_shape(hunan.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty",
          palette = "RdBu",
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)
```

### 13.1.3 Mapping local Moran's p values

The choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.

The code chunks below produce a choropleth map of Moran's I p-values by using functions of **tmap** package.

```{r}
tm_shape(hunan.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)
```

### 13.1.4 Mapping both local Moran's I values and p-values

For effective interpretation, it is better to plot both the local Moran's I values map and its corresponding p-values map next to each other.

The code chunk below will be used to create such visualisation.

```{r}
localMI.map <- tm_shape(hunan.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty", 
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)

pvalue.map <- tm_shape(hunan.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)

tmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)
```

## 14. Creating a LISA Cluster Map

The LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.

### 14.1 Plotting Moran scatterplot

The Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.

The code chunk below plots the Moran scatterplot of GDPPC 2012 by using [*moran.plot()*](https://r-spatial.github.io/spdep/reference/moran.plot.html) of **spdep**.

```{r}
nci <- moran.plot(hunan$GDPPC, rswm_q,
                  labels=as.character(hunan$County), 
                  xlab="GDPPC 2012", 
                  ylab="Spatially Lag GDPPC 2012")
```

Notice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.

### 14.2 Plotting Moran scatterplot with standardized variable

First we will use [*scale()*](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/scale) to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.

```{r}
hunan$Z.GDPPC <- scale(hunan$GDPPC) %>% as.vector 
```

The [*as.vector()*](https://www.rdocumentation.org/packages/pbdDMAT/versions/0.5-1/topics/as.vector) added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.

Now, we are ready to plot the Moran scatterplot again by using the code chunk below.

```{r}
nci2 <- moran.plot(hunan$Z.GDPPC, rswm_q,
                   labels=as.character(hunan$County),
                   xlab="z-GDPPC 2012", 
                   ylab="Spatially Lag z-GDPPC 2012")
```

### 14.3 Preparing LISA map classes

The code chunks below follow the steps to prepare a LISA cluster map.

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))
```

The code chunk below centers the variable of interest around its mean.

```{r}
DV <- hunan$GDPPC - mean(hunan$GDPPC)     
```

The code chunk below centers the local Moran's around the mean.

```{r}
C_mI <- localMI[,1] - mean(localMI[,1])    
```

The code chunk below sets the significance value/alpha to 0.05.

```{r}
signif <- 0.05       
```

These four command lines define the high-high, low-low, low-high and high-low categories.

```{r}
quadrant[DV >0 & C_mI>0] <- 4      
quadrant[DV <0 & C_mI<0] <- 2      
quadrant[DV <0 & C_mI>0] <- 1
quadrant[DV >0 & C_mI<0] <- 3
```

The code chunk below places non-significant Moran in the category 0.

```{r}
quadrant[localMI[,5]>signif] <- 0
```

Alternatively, we can combine all steps into one code chunk, as shown below:

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))
DV <- hunan$GDPPC - mean(hunan$GDPPC)     
C_mI <- localMI[,1] - mean(localMI[,1])    
signif <- 0.05       
quadrant[DV >0 & C_mI>0] <- 4      
quadrant[DV <0 & C_mI<0] <- 2      
quadrant[DV <0 & C_mI>0] <- 1
quadrant[DV >0 & C_mI<0] <- 3
quadrant[localMI[,5]>signif] <- 0
```

## 14.4 Plotting LISA map

The code chunk below plots the LISA map.

```{r}
hunan.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

tm_shape(hunan.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)
```

For effective interpretation, it is better to plot both the local Moran's I values map and its corresponding p-values map next to each other.

The code chunk below will be used to create such visualization.

```{r}
gdppc <- qtm(hunan, "GDPPC")

hunan.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

LISAmap <- tm_shape(hunan.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)

tmap_arrange(gdppc, LISAmap, asp=1, ncol=2)
```

??

## 15. Hot Spot and Cold Spot Area Analysis

Beside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.

The term 'hot spot' has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).

### 15.1 Getis and Ord's G-Statistics

An alternative spatial statistics to detect spatial anomalies is the Getis and Ord's G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.

The analysis consists of three steps:

-   Deriving spatial weight matrix

-   Computing Gi statistics

-   Mapping Gi statistics

### 15.2 Deriving distance-based weight matrix

First, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.

There are two type of distance-based proximity matrix, they are:

-   fixed distance weight matrix; and

-   adaptive distance weight matrix.

### 15.3 Deriving the centroid

We will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running *st_centroid()* on the sf object: **us.bound**. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be *st_centroid()*. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation

To get our longitude values we map the *st_centroid()* function over the geometry column of us.bound and access the longitude value through double bracket notation \[\[\]\] and 1. This allows us to get only the longitude, which is the first value in each centroid.

```{r}
longitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])
```

We do the same for latitude with one key difference. We access the second value per each centroid with \[\[2\]\].

```{r}
latitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])
```

Now that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.

```{r}
coords <- cbind(longitude, latitude)
```

### 15.4 Determine the cut-off distance

### Firstly, we need to determine the upper limit for distance band by using the steps below:

-   Return a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using [*knearneigh()*](https://r-spatial.github.io/spdep/reference/knearneigh.html) of **spdep**.

-   Convert the knn object returned by *knearneigh()* into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using [*knn2nb()*](https://r-spatial.github.io/spdep/reference/knn2nb.html).

-   Return the length of neighbour relationship edges by using [*nbdists()*](https://r-spatial.github.io/spdep/reference/nbdists.html) of **spdep**. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.

-   Remove the list structure of the returned object by using [**unlist()**](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/unlist).

```{r}
#coords <- coordinates(hunan)
k1 <- knn2nb(knearneigh(coords))
k1dists <- unlist(nbdists(k1, coords, longlat = TRUE))
summary(k1dists)
```

The summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.

### 15.5 Computing fixed distance weight matrix

Now, we will compute the distance weight matrix by using [*dnearneigh()*](https://r-spatial.github.io/spdep/reference/dnearneigh.html) as shown in the code chunk below.

```{r}
wm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)
wm_d62
```

Next, *nb2listw()* is used to convert the nb object into spatial weights object.

```{r}
wm62_lw <- nb2listw(wm_d62, style = 'B')
summary(wm62_lw)
```

The output spatial weights object is called `wm62_lw`.

### 15.6 Computing adaptive distance weight matrix

One of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.

It is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.

```{r}
knn <- knn2nb(knearneigh(coords, k=8))
knn
```

Next, *nb2listw()* is used to convert the nb object into spatial weights object.

```{r}
knn_lw <- nb2listw(knn, style = 'B')
summary(knn_lw)
```

## 17. Computing GI Statistics

### 17.1 Gi Statistics usingn fixed distance

```{r}
fips <- order(hunan$County)
gi.fixed <- localG(hunan$GDPPC, wm62_lw)
gi.fixed
```

The output of localG() is a vector of G or Gstar values, with attributes "gstari" set to TRUE or FALSE, "call" set to the function call, and class "localG".

The Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.

Next, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.

```{r}
hunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%
  rename(gstat_fixed = as.matrix.gi.fixed.)
```

In fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. *gi.fixed*) into r matrix object by using *as.matrix()*. Next, *cbind()* is used to join hunan\@data and *gi.fixed* matrix to produce a new SpatialPolygonDataFrame called *hunan.gi*. Lastly, the field name of the gi values is renamed to *gstat_fixed* by using *rename()*.

### 17.2 Mapping GI values with fixed distance weights

The code chunk below shows the functions used to map the GI values derived using fixed distance weight matrix.

```{r}
gdppc <- qtm(hunan, "GDPPC")

Gimap <-tm_shape(hunan.gi) +
  tm_fill(col = "gstat_fixed", 
          style = "pretty",
          palette="-RdBu",
          title = "local Gi") +
  tm_borders(alpha = 0.5)

tmap_arrange(gdppc, Gimap, asp=1, ncol=2)
```

??

### 17.3 Gi Statistics using adaptive distance

The code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e *knb_lw*).

```{r}
fips <- order(hunan$County)
gi.adaptive <- localG(hunan$GDPPC, knn_lw)
hunan.gi <- cbind(hunan, as.matrix(gi.adaptive)) %>%
  rename(gstat_adaptive = as.matrix.gi.adaptive.)
```

### 17.4 Mapping Gi values with adaptive distance weights

It is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of **tmap** package will be used to map the Gi values.

The code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.

```{r}
gdppc<- qtm(hunan, "GDPPC")

Gimap <- tm_shape(hunan.gi) + 
  tm_fill(col = "gstat_adaptive", 
          style = "pretty", 
          palette="-RdBu", 
          title = "local Gi") + 
  tm_borders(alpha = 0.5)

tmap_arrange(gdppc, 
             Gimap, 
             asp=1, 
             ncol=2)
```
